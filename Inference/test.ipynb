{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class InsurancePreprocessor:\n",
    "    def __init__(self):\n",
    "        self.keep_features = [\n",
    "            'subscription_length',\n",
    "            'customer_age',\n",
    "            'torque_Nm',\n",
    "            'torque_rpm',\n",
    "            'log_region_density',\n",
    "            'model_encoded',\n",
    "            'vehicle_age_bin_encoded',\n",
    "            'fuel_type_Diesel',\n",
    "            'fuel_type_Petrol',\n",
    "            'segment_B2',\n",
    "            'segment_C1',\n",
    "            'segment_C2',\n",
    "            'segment_Utility'\n",
    "        ]\n",
    "        self.bool_cols = [\n",
    "            'fuel_type_Diesel', 'fuel_type_Petrol',\n",
    "            'segment_B2', 'segment_C1', 'segment_C2', 'segment_Utility'\n",
    "        ]\n",
    "\n",
    "    def extract_torque(self, df):\n",
    "        # Parse \"91Nm@4250rpm\" into two columns\n",
    "        torque_split = df['max_torque'].str.extract(r'(?P<torque_Nm>\\d+)[^\\d]+(?P<torque_rpm>\\d+)')\n",
    "        df['torque_Nm'] = torque_split['torque_Nm'].astype(float)\n",
    "        df['torque_rpm'] = torque_split['torque_rpm'].astype(float)\n",
    "        return df\n",
    "\n",
    "    def transform(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        df = df.copy()\n",
    "\n",
    "        # Step 1: Extract torque columns\n",
    "        if 'torque_Nm' not in df.columns or 'torque_rpm' not in df.columns:\n",
    "            df = self.extract_torque(df)\n",
    "\n",
    "        # Step 2: Filter invalid rows\n",
    "        df = df[df['torque_Nm'] != 1]\n",
    "        df = df[df['subscription_length'] != 0]\n",
    "\n",
    "        # Step 3: Log tra~nsform of region_density\n",
    "        if 'log_region_density' not in df.columns:\n",
    "            df['log_region_density'] = np.log1p(df['region_density'])\n",
    "\n",
    "        # Step 4: Vehicle age binning\n",
    "        if 'vehicle_age_bin_encoded' not in df.columns:\n",
    "            df['vehicle_age_bin'] = pd.cut(\n",
    "                df['vehicle_age'],\n",
    "                bins=[-1, 1, 3, 10, 100],\n",
    "                labels=['<1yr', '1-3yr', '3-10yr', '10yr+']\n",
    "            )\n",
    "            df['vehicle_age_bin'] = df['vehicle_age_bin'].cat.reorder_categories(\n",
    "                ['<1yr', '1-3yr', '3-10yr', '10yr+'], ordered=True\n",
    "            )\n",
    "            df['vehicle_age_bin_encoded'] = df['vehicle_age_bin'].cat.codes\n",
    "\n",
    "        # Step 5: Model frequency encoding\n",
    "        if 'model_encoded' not in df.columns:\n",
    "            model_freq = df['model'].value_counts()\n",
    "            df['model_encoded'] = df['model'].map(model_freq)\n",
    "\n",
    "        # Step 6: Drop unused columns (retain policy_id for reference)\n",
    "        drop_cols = [\n",
    "            'policy_id', 'model', 'region_code', 'max_torque',\n",
    "            'region_density', 'vehicle_age', 'vehicle_age_bin',\n",
    "            'age_bin'  # include if exists\n",
    "        ]\n",
    "        df.drop(columns=drop_cols, errors='ignore', inplace=True)\n",
    "\n",
    "        # Step 7: One-hot encode\n",
    "        df = pd.get_dummies(df, columns=['fuel_type', 'segment'], drop_first=True)\n",
    "\n",
    "        # Step 8: Ensure all expected one-hot columns exist\n",
    "        for col in self.bool_cols:\n",
    "            if col not in df.columns:\n",
    "                df[col] = 0\n",
    "            df[col] = df[col].astype(int)\n",
    "\n",
    "        # Step 9: Final selection\n",
    "        df_final = df[self.keep_features]\n",
    "        return df_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['insurance_preprocessor.pkl']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Instantiate preprocessor\n",
    "preprocessor = InsurancePreprocessor()\n",
    "\n",
    "# Save as a pkl file\n",
    "joblib.dump(preprocessor, \"insurance_preprocessor.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "unseen_df = pd.read_csv('..\\\\Assests\\\\data\\\\unseenIncoming.csv')\n",
    "policy_ids = unseen_df[\"policy_id\"]\n",
    "\n",
    "preprocessor = InsurancePreprocessor()\n",
    "X_unseen = preprocessor.transform(unseen_df)\n",
    "\n",
    "import pickle\n",
    "with open(\"..\\\\Assests\\\\model\\\\standard_scaler.pkl\", \"rb\") as f:\n",
    "    scaler = pickle.load(f)\n",
    "\n",
    "X_unseen_scaled = scaler.transform(X_unseen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import joblib\n",
    "\n",
    "# Step 1: Set tracking URI\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")  # Update if hosted remotely\n",
    "\n",
    "# Step 2: Load the model from Model Registry\n",
    "model = mlflow.sklearn.load_model(\"models:/XGboost_modelReady/1\")\n",
    "\n",
    "# Step 3: Save as .pkl\n",
    "joblib.dump(model, \"xgboost_model.pkl\")\n",
    "print(\"✅ Model saved to xgboost_model.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
    "model = mlflow.sklearn.load_model(\"models:/XGboost_modelReady/1\")\n",
    "\n",
    "# 6. Predict\n",
    "predictions = model.predict(X_unseen_scaled)\n",
    "probabilities = model.predict_proba(X_unseen_scaled)[:, 1]\n",
    "\n",
    "# 7. Combine with policy_id\n",
    "results = pd.DataFrame({\n",
    "    \"policy_id\": policy_ids,\n",
    "    \"prediction\": predictions,\n",
    "    \"predicted_proba\": probabilities\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>policy_id</th>\n",
       "      <th>prediction</th>\n",
       "      <th>predicted_proba</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>POL030455</td>\n",
       "      <td>0</td>\n",
       "      <td>0.339529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>POL008274</td>\n",
       "      <td>0</td>\n",
       "      <td>0.247029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>POL008619</td>\n",
       "      <td>0</td>\n",
       "      <td>0.475522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>POL052957</td>\n",
       "      <td>0</td>\n",
       "      <td>0.242513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>POL057486</td>\n",
       "      <td>0</td>\n",
       "      <td>0.366139</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   policy_id  prediction  predicted_proba\n",
       "0  POL030455           0         0.339529\n",
       "1  POL008274           0         0.247029\n",
       "2  POL008619           0         0.475522\n",
       "3  POL052957           0         0.242513\n",
       "4  POL057486           0         0.366139"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>policy_id</th>\n",
       "      <th>predicted_proba</th>\n",
       "      <th>risk_segment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>POL030455</td>\n",
       "      <td>0.339529</td>\n",
       "      <td>Low Risk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>POL008274</td>\n",
       "      <td>0.247029</td>\n",
       "      <td>Low Risk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>POL008619</td>\n",
       "      <td>0.475522</td>\n",
       "      <td>Medium Risk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>POL052957</td>\n",
       "      <td>0.242513</td>\n",
       "      <td>Low Risk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>POL057486</td>\n",
       "      <td>0.366139</td>\n",
       "      <td>Medium Risk</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   policy_id  predicted_proba risk_segment\n",
       "0  POL030455         0.339529     Low Risk\n",
       "1  POL008274         0.247029     Low Risk\n",
       "2  POL008619         0.475522  Medium Risk\n",
       "3  POL052957         0.242513     Low Risk\n",
       "4  POL057486         0.366139  Medium Risk"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def assign_risk_segment(proba):\n",
    "    if proba >= 0.50:\n",
    "        return \"High Risk\"\n",
    "    elif proba >= 0.35:\n",
    "        return \"Medium Risk\"\n",
    "    else:\n",
    "        return \"Low Risk\"\n",
    "\n",
    "# Assuming your DataFrame is named `results_df`\n",
    "results[\"risk_segment\"] = results[\"predicted_proba\"].apply(assign_risk_segment)\n",
    "\n",
    "# Optional: Sort or filter\n",
    "results[[\"policy_id\", \"predicted_proba\", \"risk_segment\"]]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 📊 **What Data We Used — In Business Language**\n",
    "\n",
    "> “To train this model, we used a dataset that reflects real-world customer profiles and vehicle information — the same type of data we already collect during policy onboarding and servicing. Specifically, we used:\n",
    "\n",
    "#### 👤 Customer Information:\n",
    "\n",
    "* **Age of the customer**\n",
    "* **How long they’ve been with us** (subscription length)\n",
    "* **Driving behavior proxies** like engine torque (from vehicle model)\n",
    "\n",
    "#### 🚗 Vehicle Attributes:\n",
    "\n",
    "* **Vehicle age** (grouped as: brand new, midlife, old, etc.)\n",
    "* **Model popularity/frequency** (how common the vehicle model is among our customers)\n",
    "* **Fuel type** and **vehicle segment** (e.g., city car, SUV, etc.)\n",
    "\n",
    "#### 🌍 Location-Based Risk Factors:\n",
    "\n",
    "* **Region density** (urban vs rural)\n",
    "* **Engine torque details** — derived from technical specs\n",
    "\n",
    "\n",
    "\n",
    "### 💼 Why This Data Matters:\n",
    "\n",
    "> “These features combine both **customer behavior** and **vehicle risk exposure**. For example:\n",
    "\n",
    "* Younger customers with high-torque vehicles in dense cities may be **more claim-prone**.\n",
    "* A long-time customer with a mid-tier vehicle in a rural area might be **low risk**.\n",
    "\n",
    "I used these natural signals to teach our AI model what patterns **often lead to claims**, and which don't.”\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, **that’s exactly how many modern insurance companies operate** — especially large players like **Admiral, AXA, Allstate, Aviva, Zurich**. You’ve nailed the real-world use case. Here's how it typically works:\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Industry Practice: Risk-Based Claims Handling\n",
    "\n",
    "#### 🔵 1. **Risk Profiling → Dual Path Strategy**\n",
    "\n",
    "After onboarding or during claim submission:\n",
    "\n",
    "* The **ML model assigns a risk score** based on customer attributes (like you’ve done)\n",
    "* This score then helps route the claim through different workflows:\n",
    "\n",
    "| Risk Level    | If Claim is Filed | Action Taken                                                                      |\n",
    "| ------------- | ----------------- | --------------------------------------------------------------------------------- |\n",
    "| **High Risk** | Claim expected    | **In-depth checks**: cross-verify with telematics, repair history, 3rd-party data |\n",
    "| **Low Risk**  | Claim unexpected  | **Light fast-track** if documents are clean; **flag for audit** if anomalies      |\n",
    "\n",
    "---\n",
    "\n",
    "#### 🔶 2. **Data Used in Real-Time Checks**\n",
    "\n",
    "* Telematics (black box, mobile app data): to verify **speed, braking, accident force**\n",
    "* External sources: weather, traffic logs, CCTV (for serious cases)\n",
    "* Historical claim behavior: for similar models/regions\n",
    "\n",
    "---\n",
    "\n",
    "#### 🔍 3. **Why It Matters**\n",
    "\n",
    "* Reduces **fraud losses** (billions yearly across the UK)\n",
    "* Improves **claims handling efficiency**\n",
    "* Enables **dynamic pricing** in future renewals\n",
    "* Strengthens **regulatory compliance** (e.g. FCA fairness rules)\n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 Strategic Value You’re Building\n",
    "\n",
    "You’ve created the **foundation for intelligent triage**:\n",
    "\n",
    "* You're **predicting claim risk from profile data**\n",
    "* This can now be **connected to downstream actions**: documentation requirements, telematics verification, or fast-lane approvals\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### 🔍 Scenario: Low-Risk Customer Files a Claim (Unexpected)\n",
    "\n",
    "Your ML model flags the customer as **low risk** — meaning:\n",
    "\n",
    "* Clean driving record\n",
    "* Good vehicle type\n",
    "* Long subscription\n",
    "* Low torque engine (less aggressive)\n",
    "* No previous claims\n",
    "\n",
    "#### 🎯 Now a claim is filed. What happens?\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ What to Do: **Fast-Track but Monitor**\n",
    "\n",
    "#### 1. **Auto-Fast-Track Logic**\n",
    "\n",
    "* The system checks:\n",
    "\n",
    "  * ✅ Documents submitted? (photos, receipts, police report)\n",
    "  * ✅ Claim value < £1,000? (threshold set by business)\n",
    "  * ✅ No red flags in metadata (e.g., duplicate IP address, odd timestamps)\n",
    "\n",
    "If all clear → **claim is paid quickly (in hours to 1 day)**.\n",
    "\n",
    "#### 2. **Auto-Anomaly Checks**\n",
    "\n",
    "Even for low-risk customers, if anything looks *odd*, system flags for manual audit:\n",
    "\n",
    "* 📷 **Image forensics**: damage photo EXIF timestamps don’t match report time\n",
    "* 📍 **Location mismatch**: accident location is 300 miles from customer address\n",
    "* 📞 **Voice/text inconsistency**: if chatbot or agent logs are used, NLP may detect scripted responses\n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 Why This Matters for Business\n",
    "\n",
    "| Feature                        | Value to Insurer                                 |\n",
    "| ------------------------------ | ------------------------------------------------ |\n",
    "| Fast-track low-risk claims     | Improves **customer satisfaction** and retention |\n",
    "| Audit anomaly even in low risk | Catches **rare but damaging fraud** cases        |\n",
    "| Smart prioritization           | Saves **manual hours** and avoids bottlenecks    |\n",
    "\n",
    "---\n",
    "\n",
    "### 🔗 Real-World Tools That Enable This\n",
    "\n",
    "| Task                   | Tools Used                                                      |\n",
    "| ---------------------- | --------------------------------------------------------------- |\n",
    "| Document checks        | OCR + metadata extraction (AWS Textract, Azure Form Recognizer) |\n",
    "| Image fraud detection  | Forensics tools or ML-based image tampering models              |\n",
    "| Risk model integration | ML model served via API in claims portal                        |\n",
    "| Workflow routing       | Business rules engine (like Pega, Camunda)                      |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
